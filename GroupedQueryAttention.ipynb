{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wb0L5rn-wf30"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, num_kv_heads = None, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.num_kv_heads = num_kv_heads if num_kv_heads is not None else num_heads # Defaults to multi head attention\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        assert self.num_heads % self.num_kv_heads == 0, \"num_heads must be divisible by num_kv_heads\"\n",
        "\n",
        "        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n",
        "\n",
        "        # Dimension for each projection\n",
        "        self.q_proj_dim  = d_model\n",
        "        self.kv_proj_dim = self.num_kv_heads * self.head_dim\n",
        "\n",
        "        # Projections\n",
        "        self.W_q = nn.Linear(self.q_proj_dim, self.q_proj_dim, bias=False)\n",
        "        self.W_k = nn.Linear(self.q_proj_dim, self.kv_proj_dim, bias=False)\n",
        "        self.W_v = nn.Linear(self.q_proj_dim, self.kv_proj_dim, bias=False)\n",
        "        self.W_o = nn.Linear(self.q_proj_dim, self.q_proj_dim, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Applying Linear Projections\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        k = k.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
        "        v = v.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
        "\n",
        "        # Repeating k and v for grouped query attention\n",
        "        # [batch_size, seq_len, num_kv_heads, head_dim] -> [batch_size, seq_len, num_heads,  head_dim]\n",
        "        # We repeat each key and value head num_queries_per_kv number of times\n",
        "\n",
        "        if self.num_kv_heads < self.num_heads:\n",
        "            k = k.repeat_interleave(self.num_queries_per_kv, dim=2)\n",
        "            v = v.repeat_interleave(self.num_queries_per_kv, dim=2)\n",
        "\n",
        "        # [batch_size, seq_len, num_heads, head_him] -> [batch_size, num_heads, seq_len, head_dim]\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        #\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        #[batch_size, num_heads, seq_len, head_dim] -> [batch_size, seq_len, num_heads,  head_dim]\n",
        "        context = context.transpose(1,2)\n",
        "\n",
        "        context = context.contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "fywXUM_WcClH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiQUeryAttention(GroupedQueryAttention):\n",
        "    def __init__(self, d_model, num_heads, dropout = 0.1):\n",
        "        super().__init__(d_model, num_heads, num_kv_heads=1, dropout=dropout)\n",
        "\n",
        "    def forward(self,x, mask = None):\n",
        "        return super().forward(x, mask)"
      ],
      "metadata": {
        "id": "mW3zjSahjSKv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "seq_len = 10\n",
        "d_model = 512\n",
        "num_heads = 8"
      ],
      "metadata": {
        "id": "TuEXXYoRjiQV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# Standard Multi-Head Attention (MHA)\n",
        "mha = GroupedQueryAttention(d_model, num_heads, num_kv_heads=num_heads)\n",
        "\n",
        "# Grouped Query Attention (GQA) with 2 KV heads\n",
        "gqa = GroupedQueryAttention(d_model, num_heads, num_kv_heads=4)\n",
        "\n",
        "# Multi-Query Attention (MQA)\n",
        "mqa = MultiQueryAttention(d_model, num_heads)\n",
        "\n",
        "# Forward passes\n",
        "mha_output = mha(x)\n",
        "gqa_output = gqa(x)\n",
        "mqa_output = mqa(x)\n",
        "\n",
        "# Print shapes and parameter counts\n",
        "mha_params = sum(p.numel() for p in mha.parameters())\n",
        "gqa_params = sum(p.numel() for p in gqa.parameters())\n",
        "mqa_params = sum(p.numel() for p in mqa.parameters())\n",
        "\n",
        "print(f\"MHA output shape: {mha_output.shape}\")\n",
        "print(f\"GQA output shape: {gqa_output.shape}\")\n",
        "print(f\"MQA output shape: {mqa_output.shape}\")\n",
        "print(f\"MHA parameter count: {mha_params}\")\n",
        "print(f\"GQA parameter count: {gqa_params}\")\n",
        "print(f\"MQA parameter count: {mqa_params}\")\n",
        "\n",
        "# Compare parameter savings\n",
        "print(f\"GQA saves {(mha_params - gqa_params) / mha_params * 100:.2f}% parameters compared to MHA\")\n",
        "print(f\"MQA saves {(mha_params - mqa_params) / mha_params * 100:.2f}% parameters compared to MHA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1ZjwLZqjlwB",
        "outputId": "284e650d-14eb-4240-f8dd-f497c3fb4b5a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MHA output shape: torch.Size([2, 10, 512])\n",
            "GQA output shape: torch.Size([2, 10, 512])\n",
            "MQA output shape: torch.Size([2, 10, 512])\n",
            "MHA parameter count: 1048576\n",
            "GQA parameter count: 786432\n",
            "MQA parameter count: 589824\n",
            "GQA saves 25.00% parameters compared to MHA\n",
            "MQA saves 43.75% parameters compared to MHA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QZO-QEPFj4h8"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}