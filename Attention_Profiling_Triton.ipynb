{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nsu4m5nStIiQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import triton.testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TorchAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        # Initialise Projection matrices\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "        # Output Projection\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        # x is of shape [batch_size, seq_len, d_model]\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.W_q(x) # [batch_size, seq_len, d_model]\n",
        "        k = self.W_k(x) # [batch_size, seq_len, d_model]\n",
        "        v = self.W_v(x) # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Reshaping for multi-head attention\n",
        "        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, num_heads, head_dim]\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # [batch_size, seq_len, num_heads, head_dim] -> [batch_size, num_heads, seq_len, head_dim]\n",
        "        q = q.transpose(1,2)\n",
        "        k = k.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        # Computing attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim) # [batch_size, num_heads, seq_len, seq_len]\n",
        "\n",
        "        # Mask for causal attention\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax to get attention weights\n",
        "        # [batch_size, num_heads, seq_len, seq_len]\n",
        "        attention_weights = F.softmax(scores, dim = -1)\n",
        "        # Softmax along dim = -1 to determine how much attention along each key dimension\n",
        "\n",
        "        # Applying dropout to attention weights\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Applying attention weights to valyues\n",
        "        # [batch_size, num_heads, seq_len, head_dim]\n",
        "        context = torch.matmul(attention_weights, v)\n",
        "\n",
        "        # Concatenating heads\n",
        "        # [batch_size, num_heads, seq_len, head_dim] -> [batch_size, seq_len, num_heads, head_dim]\n",
        "        context = context.transpose(1, 2)\n",
        "        # [batch_size, num_heads, seq_len, head_dim] -> [batch_size, seq_len, d_model]\n",
        "        context = context.contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "Xr6v5QDytthI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating causal mask"
      ],
      "metadata": {
        "id": "St7uwCO6xwyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_causal_mask(seq_len):\n",
        "    # Lower tringular matrix so that query tokens dont have access to keys that come after them in the sequence\n",
        "    mask = torch.tril(torch.ones((seq_len, seq_len)))\n",
        "\n",
        "    return mask.unsqueeze(0).unsqueeze(0) # [1, 1, seq_len, seq_len]"
      ],
      "metadata": {
        "id": "hBCnPJjuxrWC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# benchmark_and_analyze()"
      ],
      "metadata": {
        "id": "9lq4FEQ5yts6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive triton implementation"
      ],
      "metadata": {
        "id": "hL7qtNXFCHBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def attention_kernel(\n",
        "    q_ptr, k_ptr, v_ptr, o_ptr,\n",
        "    batch_size, seq_len, num_heads, head_dim,\n",
        "    q_batch_stride, q_head_stride, q_seq_stride, q_head_dim_stride,\n",
        "    k_batch_stride, k_head_stride, k_seq_stride, k_head_dim_stride,\n",
        "    v_batch_stride, v_head_stride, v_seq_stride, v_head_dim_stride,\n",
        "    o_batch_stride, o_head_stride, o_seq_stride, o_head_dim_stride,\n",
        "    scale, # 1 / square_root(d_k)\n",
        "    BLOCK_SIZE: tl.constexpr\n",
        "    ):\n",
        "\n",
        "    batch_idx = tl.program_id(0)\n",
        "    head_idx = tl.program_id(1)\n",
        "    seq_idx =  tl.program_id(2)\n",
        "\n",
        "    # Computing pointer offsets\n",
        "    # Navigate to correct starting positions\n",
        "    q_batch_offset = batch_idx * q_batch_stride\n",
        "    q_head_offset = head_idx * q_head_stride\n",
        "    q_seq_offset = seq_idx * q_seq_stride\n",
        "\n",
        "    # No sequence offset for K and V since each query block will see all the key blocks\n",
        "    k_batch_offset = batch_idx * k_batch_stride\n",
        "    k_head_offset = head_idx * k_head_stride\n",
        "\n",
        "    v_batch_offset = batch_idx * v_batch_stride\n",
        "    v_head_offset = head_idx * v_head_stride\n",
        "\n",
        "    o_batch_offset = batch_idx * o_batch_stride\n",
        "    o_head_offset = head_idx * o_head_stride\n",
        "    o_seq_offset = seq_idx * o_seq_stride\n",
        "\n",
        "    # Loading query vector for this sequence position\n",
        "    q_ptrs = q_ptr + q_batch_offset + q_head_offset + q_seq_offset + tl.arange(0, BLOCK_SIZE) * q_head_dim_stride # This loads data even if it is in non contiguous locations in memory\n",
        "    q = tl.load(q_ptrs, mask = tl.arange(0, BLOCK_SIZE) < head_dim, other = 0.0)\n",
        "\n",
        "    # Initialise accumulator for weighted sum. One score for each key token\n",
        "    acc = tl.zeros([BLOCK_SIZE], dtype = tl.float32)\n",
        "\n",
        "    softmax_denominator = 0.0\n",
        "\n",
        "    for k_seq_idx in range(seq_len):\n",
        "        k_seq_offset = k_seq_idx * k_seq_stride\n",
        "        k_ptrs =  k_ptr + k_batch_offset + k_head_offset + k_seq_offset + tl.arange(0, BLOCK_SIZE) * k_head_dim_stride\n",
        "        k = tl.load(k_ptrs, mask = tl.arange(0, BLOCK_SIZE) < head_dim, other = 0.0)\n",
        "\n",
        "        score = tl.sum(q * k) * scale\n",
        "        attention_weight = tl.exp(score)\n",
        "\n",
        "        softmax_denominator += attention_weight\n",
        "\n",
        "        v_seq_offset = k_seq_idx * v_seq_stride\n",
        "        v_ptrs = v_ptr + v_batch_offset + v_head_offset + v_seq_offset + tl.arange(0, BLOCK_SIZE) * v_head_dim_stride\n",
        "\n",
        "        v = tl.load(v_ptrs, mask = tl.arange(0, BLOCK_SIZE) < head_dim, other = 0.0)\n",
        "\n",
        "        acc += attention_weight * v\n",
        "\n",
        "    acc /= softmax_denominator\n",
        "\n",
        "    output_ptrs = o_ptr + o_batch_offset + o_head_offset + o_seq_offset + tl.arange(0, BLOCK_SIZE) * o_head_dim_stride\n",
        "    tl.store(output_ptrs, acc, mask = tl.arange(0, BLOCK_SIZE) < head_dim)\n",
        "\n"
      ],
      "metadata": {
        "id": "f4feaH8uyvsy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TritonAttentionNaive(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        o = torch.empty_like(q)\n",
        "\n",
        "        scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        grid = (batch_size, self.num_heads, seq_len)\n",
        "\n",
        "        block_size = 1\n",
        "        while block_size < self.head_dim:\n",
        "            block_size *= 2\n",
        "\n",
        "        attention_kernel[grid](\n",
        "            q, k, v, o,\n",
        "            batch_size, seq_len, self.num_heads, self.head_dim,\n",
        "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n",
        "            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
        "            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
        "            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n",
        "            scale,\n",
        "            BLOCK_SIZE = block_size\n",
        "        )\n",
        "\n",
        "        #  [batch_size, num_heads , seq_len, head_dim] -> [batch_size, seq_len, num_heads, head_dim] -> [batch_size, seq_len, d_model]\n",
        "        o = o.permute(0,2,1,3).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        o = self.W_o(o)\n",
        "\n",
        "        return o"
      ],
      "metadata": {
        "id": "4_FypGRhKEgW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimized Multi Head Attention using Triton"
      ],
      "metadata": {
        "id": "gj-F-iWTW8Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def optimized_attention_kernel(\n",
        "      q_ptr, k_ptr, v_ptr, o_ptr,\n",
        "      batch_size, seq_len, num_heads, head_dim,\n",
        "      q_batch_stride, q_head_stride, q_seq_stride, q_head_dim_stride,\n",
        "      k_batch_stride, k_head_stride, k_seq_stride, k_head_dim_stride,\n",
        "      v_batch_stride, v_head_stride, v_seq_stride, v_head_dim_stride,\n",
        "      o_batch_stride, o_head_stride, o_seq_stride, o_head_dim_stride,\n",
        "      scale, # 1 / square_root(d_k)\n",
        "      BLOCK_SIZE_M: tl.constexpr, # Query Block Size\n",
        "      BLOCK_SIZE_N: tl.constexpr, # Key Block Size\n",
        "      BLOCK_SIZE_DMODEL : tl.constexpr, # Head Dimension Block Size,\n",
        "      USE_CAUSAL_MASK : tl.constexpr\n",
        "      ):\n",
        "\n",
        "      batch_id = tl.program_id(0)\n",
        "      head_id = tl.program_id(1)\n",
        "      seq_start =  tl.program_id(2) * BLOCK_SIZE_M\n",
        "\n",
        "      q_head_offset = head_id * q_head_stride\n",
        "      k_head_offset = head_id * k_head_stride\n",
        "      v_head_offset = head_id * v_head_stride\n",
        "\n",
        "      q_batch_offset = batch_id * q_batch_stride\n",
        "      k_batch_offset = batch_id * k_batch_stride\n",
        "      v_batch_offset = batch_id * v_batch_stride\n",
        "\n",
        "      o_head_offset = head_id * o_head_stride\n",
        "      o_batch_offset = batch_id * o_batch_stride\n",
        "\n",
        "      # Initializing accumulators\n",
        "      m_i = tl.zeros([BLOCK_SIZE_M], dtype = tl.float32) - float('inf')\n",
        "      l_i = tl.zeros([BLOCK_SIZE_M], dtype = tl.float32)\n",
        "      acc = tl.zeros([BLOCK_SIZE_M, BLOCK_SIZE_DMODEL], dtype = tl.float32)\n",
        "\n",
        "      q_block_mask = (seq_start + tl.arange(0, BLOCK_SIZE_M)) < seq_len\n",
        "\n",
        "      # Processing key blocks\n",
        "      for key_start in range(0, seq_len, BLOCK_SIZE_N):\n",
        "          k_block_mask = (key_start + tl.arange(0, BLOCK_SIZE_N)) < seq_len\n",
        "\n",
        "          if USE_CAUSAL_MASK:\n",
        "            causal_mask = tl.arange(0, BLOCK_SIZE_M)[:, None] + seq_start >= tl.arange(0, BLOCK_SIZE_N)[None, :] + key_start # Process only tokens that occur before the given query\n",
        "\n",
        "          # Loading Query Block [BLOCK_M, BLOCK_DMODEL]\n",
        "          q_block_ptr = q_ptr + q_batch_offset + q_head_offset + (seq_start + tl.arange(0, BLOCK_SIZE_M)[:, None])* q_seq_stride + (key_start + tl.arange(0, BLOCK_SIZE_DMODEL)[None, :]) * q_head_dim_stride\n",
        "\n",
        "          q_block = tl.load(q_block_ptr, mask=q_block_mask[:, None] & (tl.arange(0, BLOCK_SIZE_DMODEL)[None, :] < head_dim), other=0.0)\n",
        "\n",
        "          # Loading Key Block [BLOCK_N, BLOCK_DMODEL]\n",
        "          k_block_ptr = k_ptr + k_batch_offset + k_head_offset + (key_start + tl.arange(0,BLOCK_SIZE_N)[:, None])* k_seq_stride + (key_start + tl.arange(0, BLOCK_SIZE_DMODEL)[None, :]) * k_head_dim_stride\n",
        "\n",
        "          k_block = tl.load(k_block_ptr, mask=k_block_mask[:, None] & (tl.arange(0, BLOCK_SIZE_DMODEL)[None, :] < head_dim), other=0.0)\n",
        "\n",
        "          # Computing attention scores\n",
        "          scores = tl.dot(q_block, tl.trans(k_block)) * scale\n",
        "\n",
        "          if USE_CAUSAL_MASK:\n",
        "            scores = tl.where(causal_mask, scores, float(\"-inf\")) # This will be zeroed our during softmax\n",
        "\n",
        "          # Stable Softmax Computation\n",
        "          # 1. Computing Max for Numerical Stability\n",
        "          m_ij = tl.max(scores, axis = 1)\n",
        "\n",
        "          # 2. Updating Running Max\n",
        "          m_i_new = tl.maximum(m_i, m_ij)\n",
        "\n",
        "          # 3. Computing Exponentials with the updated max\n",
        "          exp_scores = tl.exp(scores - m_i_new[:, None])\n",
        "\n",
        "          # 4. Compute Scaling factor for previous computations\n",
        "          alpha = tl.exp(m_i - m_i_new)\n",
        "\n",
        "          # 5. Updating normalization factor\n",
        "          l_i_new = alpha * l_i + tl.sum(exp_scores, axis = 1)\n",
        "\n",
        "          # Loading Value block [BLOCK_N, BLOCK_DMODEL]\n",
        "          v_block_ptr = v_ptr + v_batch_offset + v_head_offset + (key_start + tl.arange(0, BLOCK_SIZE_N)[:, None])* v_seq_stride + (key_start + tl.arange(0, BLOCK_SIZE_DMODEL)[None, :]) * v_head_dim_stride\n",
        "\n",
        "          v_block = tl.load(v_block_ptr, mask=k_block_mask[:, None] & (tl.arange(0, BLOCK_SIZE_DMODEL)[None, :] < head_dim), other=0.0)\n",
        "\n",
        "          acc = acc * alpha[:, None] + tl.dot(exp_scores, v_block)\n",
        "\n",
        "          m_i = m_i_new\n",
        "          l_i = l_i_new\n",
        "\n",
        "      acc /= l_i[:, None]\n",
        "\n",
        "      o_block_ptr = o_ptr + o_batch_offset + o_head_offset + (seq_start + tl.arange(0, BLOCK_SIZE_M)[:, None])* o_seq_stride + (tl.arange(0, BLOCK_SIZE_DMODEL)[None, :]) * o_head_dim_stride\n",
        "\n",
        "      tl.store(o_block_ptr, acc, mask=q_block_mask[:, None] & (tl.arange(0, BLOCK_SIZE_DMODEL)[None, :] < head_dim))"
      ],
      "metadata": {
        "id": "qngNX2yKXAPs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TritonAttentionOptimized(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, causal = False):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.causal = causal\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_k =nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Prepare output tensor\n",
        "        output = torch.empty_like(q)\n",
        "\n",
        "        # Scaling factor\n",
        "        scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        BLOCK_M = 16\n",
        "        BLOCK_N = 16\n",
        "\n",
        "        # Round head_dim up to the nearest power of 2 for BLOCK_DMODEL\n",
        "        BLOCK_DMODEL = 1\n",
        "        while BLOCK_DMODEL < self.head_dim:\n",
        "            BLOCK_DMODEL *= 2\n",
        "\n",
        "        grid = (batch_size, self.num_heads, triton.cdiv(seq_len, BLOCK_M))\n",
        "\n",
        "        optimized_attention_kernel[grid](\n",
        "            q, k, v, output,\n",
        "            batch_size, seq_len, self.num_heads, self.head_dim,\n",
        "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n",
        "            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
        "            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
        "            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n",
        "            scale,\n",
        "            BLOCK_SIZE_M=BLOCK_M,\n",
        "            BLOCK_SIZE_N=BLOCK_N,\n",
        "            BLOCK_SIZE_DMODEL=BLOCK_DMODEL,\n",
        "            USE_CAUSAL_MASK=self.causal,\n",
        "        )\n",
        "\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        output = self.W_o(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "uNaoytxFewMR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flash Attention v2"
      ],
      "metadata": {
        "id": "Q9a_PlYWz-eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have hierarchial tiling, the blocks are further divided and processed as sub-blocks"
      ],
      "metadata": {
        "id": "7FZam6H602xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def flash_attn_v2_forward(\n",
        "    # Pointers to matrices\n",
        "    q_ptr, k_ptr, v_ptr, o_ptr,\n",
        "    # Matrix dimensions\n",
        "    batch_size, seq_len, num_heads, head_dim,\n",
        "    # Strides for accessing tensors\n",
        "    q_batch_stride, q_head_stride, q_seq_stride, q_head_dim_stride,\n",
        "    k_batch_stride, k_head_stride, k_seq_stride, k_head_dim_stride,\n",
        "    v_batch_stride, v_head_stride, v_seq_stride, v_head_dim_stride,\n",
        "    o_batch_stride, o_head_stride, o_seq_stride, o_head_dim_stride,\n",
        "    # Scale factor\n",
        "    scale,\n",
        "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
        "    # Causal flag\n",
        "    IS_CAUSAL: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Simplified Flash Attention V2 forward pass\n",
        "    \"\"\"\n",
        "    # Program ID\n",
        "    batch_id = tl.program_id(0)\n",
        "    head_id = tl.program_id(1)\n",
        "    m_id = tl.program_id(2)\n",
        "\n",
        "    # Starting row index\n",
        "    start_m = m_id * BLOCK_SIZE_M\n",
        "\n",
        "    q_batch_offset = batch_id * q_batch_stride\n",
        "    q_head_offset = head_id * q_head_stride\n",
        "    k_batch_offset = batch_id * k_batch_stride\n",
        "    k_head_offset = head_id * k_head_stride\n",
        "    v_batch_offset = batch_id * v_batch_stride\n",
        "    v_head_offset = head_id * v_head_stride\n",
        "    o_batch_offset = batch_id * o_batch_stride\n",
        "    o_head_offset = head_id * o_head_stride\n",
        "\n",
        "    # Initialize accumulators\n",
        "    m_i = tl.zeros([BLOCK_SIZE_M], dtype=tl.float32) - float(\"inf\")\n",
        "    l_i = tl.zeros([BLOCK_SIZE_M], dtype=tl.float32)\n",
        "    acc = tl.zeros([BLOCK_SIZE_M, BLOCK_SIZE_K], dtype=tl.float32)\n",
        "\n",
        "    # Create row indices and mask\n",
        "    row_indices = start_m + tl.arange(0, BLOCK_SIZE_M)\n",
        "    row_mask = row_indices < seq_len\n",
        "\n",
        "    # Loading Q block once - I'll reuse it for all K,V blocks\n",
        "    q_block = tl.load(\n",
        "        q_ptr + q_batch_offset + q_head_offset +\n",
        "        row_indices[:, None] * q_seq_stride +\n",
        "        tl.arange(0, BLOCK_SIZE_K)[None, :] * q_head_dim_stride,\n",
        "        mask=row_mask[:, None] & (tl.arange(0, BLOCK_SIZE_K)[None, :] < head_dim),\n",
        "        other=0.0\n",
        "    )\n",
        "\n",
        "    # Process blocks of K and V\n",
        "    for start_n in range(0, seq_len, BLOCK_SIZE_N):\n",
        "        col_indices = start_n + tl.arange(0, BLOCK_SIZE_N)\n",
        "        col_mask = col_indices < seq_len\n",
        "\n",
        "        if IS_CAUSAL:\n",
        "            causal_mask = row_indices[:, None] >= col_indices[None, :]\n",
        "\n",
        "        # Loading K block\n",
        "        k_block = tl.load(\n",
        "            k_ptr + k_batch_offset + k_head_offset +\n",
        "            col_indices[:, None] * k_seq_stride +\n",
        "            tl.arange(0, BLOCK_SIZE_K)[None, :] * k_head_dim_stride,\n",
        "            mask=col_mask[:, None] & (tl.arange(0, BLOCK_SIZE_K)[None, :] < head_dim),\n",
        "            other=0.0\n",
        "        )\n",
        "\n",
        "        scores = tl.dot(q_block, tl.trans(k_block)) * scale\n",
        "\n",
        "        if IS_CAUSAL:\n",
        "            scores = tl.where(causal_mask, scores, float('-inf'))\n",
        "\n",
        "        # Compute new max for stable softmax\n",
        "        m_i_new = tl.maximum(m_i, tl.max(scores, axis=1))\n",
        "        alpha = tl.exp(m_i - m_i_new)\n",
        "\n",
        "        # Update max values\n",
        "        m_i = m_i_new\n",
        "\n",
        "        # Compute softmax values with updated max\n",
        "        p = tl.exp(scores - m_i[:, None])\n",
        "\n",
        "        # Load V block\n",
        "        v_block = tl.load(\n",
        "            v_ptr + v_batch_offset + v_head_offset +\n",
        "            col_indices[:, None] * v_seq_stride +\n",
        "            tl.arange(0, BLOCK_SIZE_K)[None, :] * v_head_dim_stride,\n",
        "            mask=col_mask[:, None] & (tl.arange(0, BLOCK_SIZE_K)[None, :] < head_dim),\n",
        "            other=0.0\n",
        "        )\n",
        "\n",
        "        # Update sum of exponentials\n",
        "        l_i_new = alpha * l_i + tl.sum(p, axis=1)\n",
        "\n",
        "        # Update weighted sum\n",
        "        acc_new = alpha[:, None] * acc + tl.dot(p, v_block)\n",
        "\n",
        "        # Update accumulators\n",
        "        l_i = l_i_new\n",
        "        acc = acc_new\n",
        "\n",
        "    # Normalize output\n",
        "    out = acc / l_i[:, None]\n",
        "\n",
        "    # Store output\n",
        "    tl.store(\n",
        "        o_ptr + o_batch_offset + o_head_offset +\n",
        "        row_indices[:, None] * o_seq_stride +\n",
        "        tl.arange(0, BLOCK_SIZE_K)[None, :] * o_head_dim_stride,\n",
        "        out,\n",
        "        mask=row_mask[:, None] & (tl.arange(0, BLOCK_SIZE_K)[None, :] < head_dim)\n",
        "    )"
      ],
      "metadata": {
        "id": "HapgJ6AkkP1B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlashAttentionV2(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        num_heads,\n",
        "        dropout=0.0,\n",
        "        causal=False,\n",
        "        block_size=64\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.dropout = dropout\n",
        "        self.causal = causal\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # Linear projections\n",
        "        self.W_q = torch.nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = torch.nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = torch.nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_o = torch.nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        # Make sure head_dim is a power of 2\n",
        "        if not (self.head_dim & (self.head_dim - 1) == 0):\n",
        "            raise ValueError(f\"Head dimension ({self.head_dim}) must be a power of 2\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Prepare output tensor\n",
        "        output = torch.empty_like(q)\n",
        "\n",
        "        # Scaling factor\n",
        "        scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Calculate grid dimensions\n",
        "        grid = (\n",
        "            batch_size,\n",
        "            self.num_heads,\n",
        "            triton.cdiv(seq_len, self.block_size)\n",
        "        )\n",
        "\n",
        "        # Round head_dim up to the nearest power of 2 if needed\n",
        "        block_k = self.head_dim\n",
        "        if block_k & (block_k - 1) != 0:\n",
        "            block_k = 1\n",
        "            while block_k < self.head_dim:\n",
        "                block_k *= 2\n",
        "\n",
        "        # Launch kernel\n",
        "        flash_attn_v2_forward[grid](\n",
        "            q, k, v, output,\n",
        "            batch_size, seq_len, self.num_heads, self.head_dim,\n",
        "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n",
        "            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
        "            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
        "            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n",
        "            scale,\n",
        "            BLOCK_SIZE_M=self.block_size,\n",
        "            BLOCK_SIZE_N=self.block_size,\n",
        "            BLOCK_SIZE_K=block_k,\n",
        "            IS_CAUSAL=self.causal,\n",
        "        )\n",
        "\n",
        "        # Reshape output back\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        # Apply dropout\n",
        "        if self.dropout > 0.0 and self.training:\n",
        "            output = torch.nn.functional.dropout(output, p=self.dropout)\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.W_o(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "o-nFY-i3kWL1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn"
      ],
      "metadata": {
        "id": "5Y0pNe9sdRDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c25baba-79ce-4ee5-9aff-05a26ad48cb9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m183.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.5.1+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->flash-attn)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->flash-attn)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->flash-attn)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->flash-attn)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->flash-attn)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->flash-attn)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/67/42/f4f60238e8194a3106d06a058d494b18e006c10bb2b915655bd9f6ea4cb1/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187815463 sha256=d944fc7d2f962bce83fc4708c2fc0c21eaf8255962a0b350ae919362a51b7ef2\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, flash-attn\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed flash-attn-2.7.4.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def gqa_forward(\n",
        "    q_ptr, k_ptr, v_ptr, o_ptr,\n",
        "    batch_size, seq_len, num_q_heads, num_kv_heads, head_dim,\n",
        "    q_batch_stride, q_head_stride, q_seq_stride, q_head_dim_stride,\n",
        "    k_batch_stride, k_head_stride, k_seq_stride, k_head_dim_stride,\n",
        "    v_batch_stride, v_head_stride, v_seq_stride, v_head_dim_stride,\n",
        "    o_batch_stride, o_head_stride, o_seq_stride, o_head_dim_stride,\n",
        "    scale,\n",
        "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
        "    num_queries_per_kv: tl.constexpr,\n",
        "    IS_CAUSAL: tl.constexpr):\n",
        "\n",
        "    batch_id = tl.program_id(0)\n",
        "    head_id = tl.program_id(1)\n",
        "    seq_id = tl.program_id(2)\n",
        "\n",
        "    start_m = seq_id * BLOCK_SIZE_M\n",
        "\n",
        "    head_kv_id = head_id // num_queries_per_kv\n",
        "\n",
        "    q_batch_offset = batch_id * q_batch_stride\n",
        "    k_batch_offset = batch_id * k_batch_stride\n",
        "    v_batch_offset = batch_id * v_batch_stride\n",
        "    o_batch_offset = batch_id * o_batch_stride\n",
        "\n",
        "    q_head_offset = head_id * q_head_stride\n",
        "    k_head_offset = head_kv_id * k_head_stride\n",
        "    v_head_offset = head_kv_id * v_head_stride\n",
        "    o_head_offset = head_id * o_head_stride\n",
        "\n",
        "    # Initialise accumulators\n",
        "    m_i = tl.zeros([BLOCK_SIZE_M], dtype=tl.float32) - float('inf')\n",
        "    l_i = tl.zeros([BLOCK_SIZE_M], dtype=tl.float32)\n",
        "    acc = tl.zeros([BLOCK_SIZE_M, BLOCK_SIZE_K], dtype=tl.float32)\n",
        "\n",
        "    row_indices = start_m + tl.arange(0, BLOCK_SIZE_M)\n",
        "    row_mask = row_indices < seq_len\n",
        "\n",
        "    # Loading a query block of size [BLOCK_SIZE_M, BLOCK_SIZE_K]\n",
        "    q_block = tl.load(\n",
        "        q_ptr + q_batch_offset + q_head_offset +\n",
        "        row_indices[:, None] * q_seq_stride +\n",
        "        tl.arange(0, BLOCK_SIZE_K)[None, :] * q_head_dim_stride,\n",
        "        mask=row_mask[:, None] & (tl.arange(0, BLOCK_SIZE_K)[None, :] < head_dim),\n",
        "        other=0.0\n",
        "    )\n",
        "\n",
        "    # Processing blocks of K and V\n",
        "    for start_n in range(0, seq_len, BLOCK_SIZE_N):\n",
        "        col_indices = start_n + tl.arange(0, BLOCK_SIZE_N)\n",
        "        col_mask = col_indices < seq_len\n",
        "\n",
        "        if IS_CAUSAL:\n",
        "            causal_mask = row_indices[:, None] >= col_indices[None, :]\n",
        "\n",
        "        # Loading a key block of size [BLOCK_SIZE_N, BLOCK_SIZE_K]\n",
        "        k_block = tl.load(\n",
        "            k_ptr + k_batch_offset + k_head_offset +\n",
        "            col_indices[:, None] * k_seq_stride +\n",
        "            tl.arange(0, BLOCK_SIZE_K)[None, :] * k_head_dim_stride,\n",
        "            mask=col_mask[:, None] & (tl.arange(0, BLOCK_SIZE_K)[None, :] < head_dim),\n",
        "            other=0.0\n",
        "        )\n",
        "\n",
        "        scores = tl.dot(q_block, tl.trans(k_block)) * scale\n",
        "\n",
        "        if IS_CAUSAL:\n",
        "            scores = tl.where(causal_mask, scores, float('-inf'))\n",
        "\n",
        "        # Stable Online Softmax\n",
        "        m_i_new = tl.maximum(m_i, tl.max(scores, axis=1))\n",
        "\n",
        "        # Scaling factor\n",
        "        alpha = tl.exp(m_i - m_i_new)\n",
        "\n",
        "        # Updating the max value\n",
        "        m_i = m_i_new\n",
        "\n",
        "        p = tl.exp(scores - m_i[:, None])\n",
        "\n",
        "        v_block = tl.load(\n",
        "            v_ptr + v_batch_offset + v_head_offset +\n",
        "            col_indices[:, None] * v_seq_stride +\n",
        "            tl.arange(0, BLOCK_SIZE_K)[None, :] * v_head_dim_stride,\n",
        "            mask=col_mask[:, None] & (tl.arange(0, BLOCK_SIZE_K)[None, :] < head_dim),\n",
        "            other=0.0\n",
        "        )\n",
        "\n",
        "        l_i_new = alpha * l_i + tl.sum(p, axis=1)\n",
        "\n",
        "        acc_new = alpha[:, None] * acc + tl.dot(p, v_block)\n",
        "\n",
        "        l_i = l_i_new\n",
        "        acc = acc_new\n",
        "\n",
        "    # Scaling by normalization factor\n",
        "    out = acc / l_i[:, None]\n",
        "\n",
        "    tl.store(\n",
        "        o_ptr + o_batch_offset + o_head_offset +\n",
        "        row_indices[:, None] * o_seq_stride +\n",
        "        tl.arange(0, BLOCK_SIZE_K)[None, :] * o_head_dim_stride,\n",
        "        out,\n",
        "        mask=row_mask[:, None] & (tl.arange(0, BLOCK_SIZE_K)[None, :] < head_dim)\n",
        "    )"
      ],
      "metadata": {
        "id": "TWcZj05d3o0Z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Triton_GQA(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, num_kv_heads=None, dropout=0.1, causal=False, block_size=64):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.num_kv_heads = num_kv_heads if num_kv_heads is not None else num_heads\n",
        "\n",
        "        assert num_heads % self.num_kv_heads == 0, \"num_heads must be divisible by num_kv_heads\"\n",
        "\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n",
        "\n",
        "        self.q_proj_dim = d_model\n",
        "        self.kv_proj_dim = self.num_kv_heads * self.head_dim\n",
        "\n",
        "        self.causal = causal\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, self.q_proj_dim, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, self.kv_proj_dim, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, self.kv_proj_dim, bias=False)\n",
        "        self.W_o = nn.Linear(self.q_proj_dim, d_model, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        k = k.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
        "        v = v.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
        "\n",
        "        # [batch_size, seq_len, num_heads, head_dim] -> [batch_size, num_heads, seq_len, head_dim]\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        output = torch.empty_like(q)\n",
        "\n",
        "        scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        grid = (\n",
        "            batch_size,\n",
        "            self.num_heads,\n",
        "            triton.cdiv(seq_len, self.block_size)\n",
        "        )\n",
        "\n",
        "        # Ensure block_k is a power of 2\n",
        "        block_k = 1\n",
        "        while block_k < self.head_dim:\n",
        "            block_k *= 2\n",
        "\n",
        "        # Launch kernel\n",
        "        gqa_forward[grid](\n",
        "            q, k, v, output,\n",
        "            batch_size, seq_len, self.num_heads, self.num_kv_heads, self.head_dim,\n",
        "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n",
        "            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
        "            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
        "            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n",
        "            scale,\n",
        "            BLOCK_SIZE_M=self.block_size,\n",
        "            BLOCK_SIZE_N=self.block_size,\n",
        "            BLOCK_SIZE_K=block_k,\n",
        "            num_queries_per_kv=self.num_queries_per_kv,\n",
        "            IS_CAUSAL=self.causal,\n",
        "        )\n",
        "\n",
        "        output = output.permute(0, 2, 1, 3)\n",
        "        output = output.contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        if self.dropout.p > 0.0 and self.training:\n",
        "            output = self.dropout(output)\n",
        "\n",
        "        output = self.W_o(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "z9Kdf-V93qp_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from flash_attn import flash_attn_func\n",
        "    FLASH_ATTN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"flash-attn library not found. Will skip that implementation.\")\n",
        "    FLASH_ATTN_AVAILABLE = False"
      ],
      "metadata": {
        "id": "kSH2dRKgdYbl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "DEVICE = torch.device('cuda')\n",
        "D_MODEL = 768\n",
        "NUM_HEADS = 12\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# flash-attn library typically uses these block sizes\n",
        "FLASH_BLOCK_SIZE = 64\n",
        "FLASH_SUB_BLOCK_SIZE = 16"
      ],
      "metadata": {
        "id": "0m46mOjReAcn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.testing.perf_report(\n",
        "    triton.testing.Benchmark(\n",
        "        x_names=['seq_len'],\n",
        "        x_vals=[2**i for i in range(8, 13)],  # 256 to 4096\n",
        "        x_log=True,\n",
        "        line_arg='provider',\n",
        "        line_vals=['torch', 'triton_naive', 'triton_optimized', 'flash_v2', 'flash_attn_lib', 'Triton_GQA'],\n",
        "        # line_vals=['torch', 'triton_naive', 'triton_optimized', 'flash_v2'],\n",
        "        line_names=['PyTorch', 'Triton Naive', 'Triton Optimized', 'My Flash V2', 'Flash-Attn Lib', 'Triton_GQA'],\n",
        "        # line_names=['PyTorch', 'Triton Naive', 'Triton Optimized', 'My Flash V2'],\n",
        "        styles=[('blue', '-'), ('green', '-'), ('red', '-'), ('purple', '-'), ('orange', '-'), ('yellow', '-')],\n",
        "        # styles=[('blue', '-'), ('green', '-'), ('red', '-'), ('purple', '-')],\n",
        "        # ylabel='Latency (ms)',\n",
        "        plot_name='attention-performance',\n",
        "        args={'d_model': D_MODEL, 'num_heads': NUM_HEADS, 'batch_size': BATCH_SIZE},\n",
        "    )\n",
        ")\n",
        "def benchmark_attention(seq_len, provider, d_model, num_heads, batch_size):\n",
        "    \"\"\"Benchmark different attention implementations.\"\"\"\n",
        "    x = torch.randn((batch_size, seq_len, d_model), device=DEVICE)\n",
        "\n",
        "    # Create the appropriate model based on provider\n",
        "    if provider == 'torch':\n",
        "        model = TorchAttention(d_model, num_heads).to(DEVICE)\n",
        "        run_func = lambda: model(x)\n",
        "\n",
        "    elif provider == 'triton_naive':\n",
        "        model = TritonAttentionNaive(d_model, num_heads).to(DEVICE)\n",
        "        run_func = lambda: model(x)\n",
        "\n",
        "    elif provider == 'triton_optimized':\n",
        "        model = TritonAttentionOptimized(d_model, num_heads, causal=False).to(DEVICE)\n",
        "        run_func = lambda: model(x)\n",
        "\n",
        "    elif provider == 'flash_v2':\n",
        "        model = FlashAttentionV2(\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            block_size=FLASH_BLOCK_SIZE\n",
        "        ).to(DEVICE)\n",
        "        run_func = lambda: model(x)\n",
        "\n",
        "    elif provider == 'flash_attn_lib':\n",
        "        if not FLASH_ATTN_AVAILABLE:\n",
        "            return float('inf'), float('inf'), float('inf')\n",
        "\n",
        "        # For the flash-attn library, prepare inputs differently\n",
        "        q_proj = torch.nn.Linear(d_model, d_model, bias=False).to(DEVICE)\n",
        "        k_proj = torch.nn.Linear(d_model, d_model, bias=False).to(DEVICE)\n",
        "        v_proj = torch.nn.Linear(d_model, d_model, bias=False).to(DEVICE)\n",
        "\n",
        "        q = q_proj(x).view(batch_size, seq_len, num_heads, d_model // num_heads)\n",
        "        k = k_proj(x).view(batch_size, seq_len, num_heads, d_model // num_heads)\n",
        "        v = v_proj(x).view(batch_size, seq_len, num_heads, d_model // num_heads)\n",
        "\n",
        "        q = q.to(torch.float16)\n",
        "        k = k.to(torch.float16)\n",
        "        v = v.to(torch.float16)\n",
        "\n",
        "        run_func = lambda: flash_attn_func(q, k, v, causal=False)\n",
        "\n",
        "    elif provider == 'Triton_GQA':\n",
        "        model = Triton_GQA(d_model, num_heads, causal=False).to(DEVICE)\n",
        "        run_func = lambda: model(x)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown provider: {provider}\")\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(3):\n",
        "        _ = run_func()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Benchmark\n",
        "    quantiles = [0.5, 0.2, 0.8]\n",
        "    ms, min_ms, max_ms = triton.testing.do_bench(run_func, quantiles=quantiles)\n",
        "\n",
        "    return ms, max_ms, min_ms"
      ],
      "metadata": {
        "id": "K36w3smghgqQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_attention.run(save_path='./attention_benchmark', print_data=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "OAnnBtAJi9Pn",
        "outputId": "ef762272-099e-4d1c-ba69-e1303f7c6836"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention-performance:\n",
            "   seq_len    PyTorch  Triton Naive  Triton Optimized  My Flash V2  Flash-Attn Lib  Triton_GQA\n",
            "0    256.0   0.263168      1.003520          0.218112     0.196608        0.022528    0.199680\n",
            "1    512.0   0.563200      3.634176          0.382976     0.301056        0.046080    0.305152\n",
            "2   1024.0   2.368000     14.093824          0.928768     0.640000        0.092160    0.643584\n",
            "3   2048.0   8.347136     58.723328          3.185664     1.775616        0.251904    1.691648\n",
            "4   4096.0  31.051777    229.123077         10.780672     5.007360        0.857088    5.009408\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uAYu-cSvlof8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}